{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48809d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nkosinathikhumalo/hackathon/robots/cv-pipeline/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'inv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m image_processor = AutoProcessor.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33musyd-community/vitpose-plus-small\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m model = VitPoseForPoseEstimation.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33musyd-community/vitpose-plus-small\u001b[39m\u001b[33m\"\u001b[39m, device_map=device)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m inputs = \u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mperson_boxes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     54\u001b[39m     outputs = model(**inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackathon/robots/cv-pipeline/.venv/lib/python3.12/site-packages/transformers/image_processing_utils.py:51\u001b[39m, in \u001b[36mBaseImageProcessor.__call__\u001b[39m\u001b[34m(self, images, **kwargs)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, **kwargs) -> BatchFeature:\n\u001b[32m     50\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackathon/robots/cv-pipeline/.venv/lib/python3.12/site-packages/transformers/models/vitpose/image_processing_vitpose.py:524\u001b[39m, in \u001b[36mVitPoseImageProcessor.preprocess\u001b[39m\u001b[34m(self, images, boxes, do_affine_transform, size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format)\u001b[39m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m box \u001b[38;5;129;01min\u001b[39;00m image_boxes:\n\u001b[32m    518\u001b[39m         center, scale = box_to_center_and_scale(\n\u001b[32m    519\u001b[39m             box,\n\u001b[32m    520\u001b[39m             image_width=size[\u001b[33m\"\u001b[39m\u001b[33mwidth\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    521\u001b[39m             image_height=size[\u001b[33m\"\u001b[39m\u001b[33mheight\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    522\u001b[39m             normalize_factor=\u001b[38;5;28mself\u001b[39m.normalize_factor,\n\u001b[32m    523\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m         transformed_image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maffine_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotation\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m         new_images.append(transformed_image)\n\u001b[32m    528\u001b[39m images = new_images\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackathon/robots/cv-pipeline/.venv/lib/python3.12/site-packages/transformers/models/vitpose/image_processing_vitpose.py:417\u001b[39m, in \u001b[36mVitPoseImageProcessor.affine_transform\u001b[39m\u001b[34m(self, image, center, scale, rotation, size, data_format, input_data_format)\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;66;03m# input image requires channels last format\u001b[39;00m\n\u001b[32m    412\u001b[39m image = (\n\u001b[32m    413\u001b[39m     image\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m input_data_format == ChannelDimension.LAST\n\u001b[32m    415\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m to_channel_dimension_format(image, ChannelDimension.LAST, input_data_format)\n\u001b[32m    416\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m image = \u001b[43mscipy_warp_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m image = to_channel_dimension_format(image, data_format, ChannelDimension.LAST)\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hackathon/robots/cv-pipeline/.venv/lib/python3.12/site-packages/transformers/models/vitpose/image_processing_vitpose.py:313\u001b[39m, in \u001b[36mscipy_warp_affine\u001b[39m\u001b[34m(src, M, size)\u001b[39m\n\u001b[32m    311\u001b[39m M_scipy = np.vstack([M, [\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m]])\n\u001b[32m    312\u001b[39m \u001b[38;5;66;03m# If you have a matrix for the ‘push’ transformation, use its inverse (numpy.linalg.inv) in this function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m M_inv = \u001b[43minv\u001b[49m(M_scipy)\n\u001b[32m    314\u001b[39m M_inv[\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m], M_inv[\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m], M_inv[\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m], M_inv[\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m], M_inv[\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m], M_inv[\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m] = (\n\u001b[32m    315\u001b[39m     M_inv[\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m],\n\u001b[32m    316\u001b[39m     M_inv[\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m     M_inv[\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m],\n\u001b[32m    321\u001b[39m )\n\u001b[32m    323\u001b[39m new_src = [affine_transform(channel, M_inv, output_shape=size, order=\u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m channel \u001b[38;5;129;01min\u001b[39;00m channels]\n",
      "\u001b[31mNameError\u001b[39m: name 'inv' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    RTDetrForObjectDetection,\n",
    "    VitPoseForPoseEstimation,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000000139.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Stage 1. Detect humans on the image\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# You can choose detector by your choice\n",
    "person_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\n",
    "person_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=device)\n",
    "\n",
    "inputs = person_image_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = person_model(**inputs)\n",
    "\n",
    "results = person_image_processor.post_process_object_detection(\n",
    "    outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.3\n",
    ")\n",
    "result = results[0]  # take first image results\n",
    "\n",
    "# Human label refers 0 index in COCO dataset\n",
    "person_boxes = result[\"boxes\"][result[\"labels\"] == 0]\n",
    "person_boxes = person_boxes.cpu().numpy()\n",
    "\n",
    "# Convert boxes from VOC (x1, y1, x2, y2) to COCO (x1, y1, w, h) format\n",
    "person_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0]\n",
    "person_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Stage 2. Detect keypoints for each person found\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "image_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-plus-small\")\n",
    "model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-plus-small\", device_map=device)\n",
    "\n",
    "inputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "pose_results = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes], threshold=0.3)\n",
    "image_pose_result = pose_results[0]  # results for first image\n",
    "\n",
    "for i, person_pose in enumerate(image_pose_result):\n",
    "    print(f\"Person #{i}\")\n",
    "    for keypoint, label, score in zip(\n",
    "        person_pose[\"keypoints\"], person_pose[\"labels\"], person_pose[\"scores\"]\n",
    "    ):\n",
    "        keypoint_name = model.config.id2label[label.item()]\n",
    "        x, y = keypoint\n",
    "        print(f\" - {keypoint_name}: x={x.item():.2f}, y={y.item():.2f}, score={score.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uk0h5v428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Define skeleton connections for COCO keypoints\n",
    "skeleton = [\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4),  # Head\n",
    "    (5, 6),  # Shoulders\n",
    "    (5, 7), (7, 9),  # Left arm\n",
    "    (6, 8), (8, 10),  # Right arm\n",
    "    (5, 11), (6, 12),  # Torso\n",
    "    (11, 12),  # Hips\n",
    "    (11, 13), (13, 15),  # Left leg\n",
    "    (12, 14), (14, 16),  # Right leg\n",
    "]\n",
    "\n",
    "# Colors for different body parts\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(skeleton)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.imshow(image)\n",
    "\n",
    "for person_pose in image_pose_result:\n",
    "    keypoints = person_pose[\"keypoints\"].cpu().numpy()\n",
    "    scores = person_pose[\"scores\"].cpu().numpy()\n",
    "    \n",
    "    # Draw skeleton connections\n",
    "    for idx, (start, end) in enumerate(skeleton):\n",
    "        if scores[start] > 0.3 and scores[end] > 0.3:\n",
    "            x_start, y_start = keypoints[start]\n",
    "            x_end, y_end = keypoints[end]\n",
    "            ax.plot([x_start, x_end], [y_start, y_end], \n",
    "                   color=colors[idx], linewidth=2, alpha=0.8)\n",
    "    \n",
    "    # Draw keypoints\n",
    "    for i, (kp, score) in enumerate(zip(keypoints, scores)):\n",
    "        if score > 0.3:\n",
    "            x, y = kp\n",
    "            ax.scatter(x, y, c='red', s=40, zorder=5)\n",
    "            ax.scatter(x, y, c='white', s=15, zorder=6)\n",
    "\n",
    "ax.set_title(\"Pose Estimation with ViTPose\", fontsize=14)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
